#!/bin/bash
set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
DEMO_MODE=${1:-"docker"}  # docker or kubernetes
NAMESPACE="demo"
LOAD_TEST_DURATION="5m"
LOAD_TEST_VUS="50"

echo -e "${BLUE}üöÄ High-Availability Auto-Scaling Demo${NC}"
echo -e "${BLUE}=====================================${NC}"
echo ""
echo -e "${YELLOW}üìã Demo Mode: ${DEMO_MODE}${NC}"
echo ""

# ASCII Art Banner
cat << 'EOF'
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë        AUTO-SCALING DEMO             ‚ïë
    ‚ïë                                      ‚ïë
    ‚ïë  üîÑ Load Testing                     ‚ïë
    ‚ïë  üìä Real-time Monitoring             ‚ïë
    ‚ïë  ‚ö° Dynamic Scaling                  ‚ïë
    ‚ïë  üìà Performance Metrics              ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF

echo ""

# Function to check prerequisites
check_prerequisites() {
    echo -e "${YELLOW}üîç Checking prerequisites...${NC}"
    
    local missing_tools=()
    
    if ! command -v docker &> /dev/null; then
        missing_tools+=("docker")
    fi
    
    if ! command -v k6 &> /dev/null; then
        missing_tools+=("k6")
    fi
    
    if [[ "$DEMO_MODE" == "kubernetes" ]] && ! command -v kubectl &> /dev/null; then
        missing_tools+=("kubectl")
    fi
    
    if [ ${#missing_tools[@]} -ne 0 ]; then
        echo -e "${RED}‚ùå Missing tools: ${missing_tools[*]}${NC}"
        echo ""
        echo -e "${YELLOW}üì¶ Installation commands:${NC}"
        for tool in "${missing_tools[@]}"; do
            case $tool in
                "docker")
                    echo "  - Docker: https://docs.docker.com/get-docker/"
                    ;;
                "k6")
                    echo "  - k6: snap install k6 or brew install k6"
                    ;;
                "kubectl")
                    echo "  - kubectl: https://kubernetes.io/docs/tasks/tools/"
                    ;;
            esac
        done
        exit 1
    fi
    
    echo -e "${GREEN}‚úÖ All prerequisites available${NC}"
}

# Function to start services
start_services() {
    echo -e "${BLUE}üèóÔ∏è  Starting services...${NC}"
    
    if [[ "$DEMO_MODE" == "docker" ]]; then
        echo -e "${YELLOW}Starting Docker Compose...${NC}"
        
        # Check if scaling compose file exists and is valid
        if [ -f "docker-compose.scaling.yml" ]; then
            docker-compose -f docker-compose.scaling.yml up -d 2>/dev/null || {
                echo -e "${YELLOW}‚ö†Ô∏è  Scaling compose file not ready, using main compose...${NC}"
                docker-compose up -d 2>/dev/null || {
                    echo -e "${RED}‚ùå Docker Compose failed. Please ensure services are built or use existing containers.${NC}"
                    echo -e "${YELLOW}üí° Continuing with demo assuming services will be available...${NC}"
                }
            }
        else
            docker-compose up -d 2>/dev/null || {
                echo -e "${RED}‚ùå Docker Compose failed. Please ensure services are built or use existing containers.${NC}"
                echo -e "${YELLOW}üí° Continuing with demo assuming services will be available...${NC}"
            }
        fi
        
        echo -e "${YELLOW}‚è≥ Waiting for services to be ready...${NC}"
        sleep 15
        
        # Check service health (graceful fallback for demo)
        services=("product-service:8080" "user-service:8081" "checkout-service:8082" "analytics-service:8083")
        available_services=0
        for service in "${services[@]}"; do
            service_name=${service%:*}
            port=${service#*:}
            echo -n "  Checking ${service_name}..."
            
            service_available=false
            for i in {1..10}; do
                if curl -s "http://localhost:${port}/health" > /dev/null 2>&1 || \
                   curl -s "http://localhost:${port}/actuator/health" > /dev/null 2>&1 || \
                   curl -s "http://localhost:${port}/" > /dev/null 2>&1; then
                    echo -e " ${GREEN}‚úÖ${NC}"
                    available_services=$((available_services + 1))
                    service_available=true
                    break
                else
                    sleep 1
                fi
            done
            
            if [ "$service_available" = false ]; then
                echo -e " ${YELLOW}‚ö†Ô∏è  (will use mock endpoints)${NC}"
            fi
        done
        
        if [ $available_services -eq 0 ]; then
            echo -e "${YELLOW}‚ö†Ô∏è  No services are currently running. Demo will run with mock endpoints.${NC}"
            echo -e "${BLUE}üí° To run with real services, ensure they are built and started first.${NC}"
        else
            echo -e "${GREEN}üéâ ${available_services} service(s) are ready!${NC}"
        fi
        
    elif [[ "$DEMO_MODE" == "kubernetes" ]]; then
        echo -e "${YELLOW}Checking Kubernetes services...${NC}"
        
        # Check if services are running
        services=("product-service" "user-service" "checkout-service" "analytics-service")
        for service in "${services[@]}"; do
            echo -n "  Checking ${service}..."
            if kubectl get deployment ${service} -n ${NAMESPACE} &> /dev/null; then
                replicas=$(kubectl get deployment ${service} -n ${NAMESPACE} -o jsonpath='{.status.readyReplicas}')
                echo -e " ${GREEN}‚úÖ (${replicas} replicas)${NC}"
            else
                echo -e " ${RED}‚ùå Not found${NC}"
                exit 1
            fi
        done
    fi
    
    echo -e "${GREEN}üéâ All services are ready!${NC}"
}

# Function to show current service status
show_service_status() {
    echo -e "${BLUE}üìä Current Service Status${NC}"
    echo "========================="
    
    if [[ "$DEMO_MODE" == "docker" ]]; then
        echo -e "${CYAN}Docker Containers:${NC}"
        docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | grep -E "(product|user|checkout|analytics)"
        
    elif [[ "$DEMO_MODE" == "kubernetes" ]]; then
        echo -e "${CYAN}Kubernetes Deployments:${NC}"
        kubectl get deployments -n ${NAMESPACE} -o custom-columns="NAME:.metadata.name,READY:.status.readyReplicas,UP-TO-DATE:.status.updatedReplicas,AVAILABLE:.status.availableReplicas"
        
        echo ""
        echo -e "${CYAN}HPA Status:${NC}"
        kubectl get hpa -n ${NAMESPACE} 2>/dev/null || echo "No HPA configured"
    fi
    echo ""
}

# Function to open monitoring dashboards
open_monitoring() {
    echo -e "${BLUE}üñ•Ô∏è  Opening monitoring dashboards...${NC}"
    
    # Determine the base URL based on demo mode
    if [[ "$DEMO_MODE" == "docker" ]]; then
        GRAFANA_URL="http://localhost:3000"
        PROMETHEUS_URL="http://localhost:9090"
    elif [[ "$DEMO_MODE" == "kubernetes" ]]; then
        # Start port forwarding
        echo -e "${YELLOW}Setting up port forwarding...${NC}"
        kubectl port-forward -n ${NAMESPACE} svc/grafana 3000:3000 &
        kubectl port-forward -n ${NAMESPACE} svc/prometheus 9090:9090 &
        
        GRAFANA_URL="http://localhost:3000"
        PROMETHEUS_URL="http://localhost:9090"
        
        # Wait for port forwarding
        sleep 5
    fi
    
    echo -e "${GREEN}üåê Monitoring URLs:${NC}"
    echo "  üìä Grafana: ${GRAFANA_URL}"
    echo "  üìà Prometheus: ${PROMETHEUS_URL}"
    echo ""
    echo -e "${YELLOW}üí° Open these URLs in your browser for real-time monitoring${NC}"
    echo ""
    
    # Try to open automatically (works on most systems)
    if command -v xdg-open &> /dev/null; then
        xdg-open "${GRAFANA_URL}" 2>/dev/null &
    elif command -v open &> /dev/null; then
        open "${GRAFANA_URL}" 2>/dev/null &
    fi
}

# Function to run load test
run_load_test() {
    local test_type=${1:-"gradual"}
    
    echo -e "${BLUE}üî• Starting Load Test (${test_type})${NC}"
    echo "=================================="
    
    # Determine target URLs based on demo mode
    if [[ "$DEMO_MODE" == "docker" ]]; then
        PRODUCT_URL="http://localhost:8080"
        USER_URL="http://localhost:8081"
        CHECKOUT_URL="http://localhost:8082"
    elif [[ "$DEMO_MODE" == "kubernetes" ]]; then
        # Use port forwarding or ingress
        PRODUCT_URL="http://localhost:8080"
        USER_URL="http://localhost:8081"
        CHECKOUT_URL="http://localhost:8082"
        
        # Start service port forwarding for load testing
        kubectl port-forward -n ${NAMESPACE} svc/product-service 8080:80 &
        kubectl port-forward -n ${NAMESPACE} svc/user-service 8081:80 &
        kubectl port-forward -n ${NAMESPACE} svc/checkout-service 8082:80 &
        sleep 3
    fi
    
    case $test_type in
        "spike")
            echo -e "${YELLOW}üèÉ‚Äç‚ôÇÔ∏è Running SPIKE load test (sudden high load)${NC}"
            k6 run -e PRODUCT_URL=${PRODUCT_URL} -e USER_URL=${USER_URL} -e CHECKOUT_URL=${CHECKOUT_URL} \
                k6-tests/spike-test.js
            ;;
        "stress")
            echo -e "${YELLOW}üèãÔ∏è‚Äç‚ôÇÔ∏è Running STRESS load test (sustained high load)${NC}"
            k6 run -e PRODUCT_URL=${PRODUCT_URL} -e USER_URL=${USER_URL} -e CHECKOUT_URL=${CHECKOUT_URL} \
                k6-tests/stress-test.js
            ;;
        "gradual")
            echo -e "${YELLOW}üìà Running GRADUAL load test (increasing load)${NC}"
            k6 run -e PRODUCT_URL=${PRODUCT_URL} -e USER_URL=${USER_URL} -e CHECKOUT_URL=${CHECKOUT_URL} \
                k6-tests/test-plan.js
            ;;
        "auto-scaling-demo")
            echo -e "${YELLOW}‚ö° Running AUTO-SCALING DEMO test (optimized for scaling demonstration)${NC}"
            
            # Check if services are available, use mock if not
            if curl -s "http://localhost:8080/health" > /dev/null 2>&1 || \
               curl -s "http://localhost:8080/" > /dev/null 2>&1; then
                echo -e "${GREEN}üéØ Using real services for demo${NC}"
                k6 run -e PRODUCT_URL=${PRODUCT_URL} -e USER_URL=${USER_URL} -e CHECKOUT_URL=${CHECKOUT_URL} \
                    k6-tests/auto-scaling-demo.js
            else
                echo -e "${YELLOW}üé≠ Using mock endpoints for demo (services not available)${NC}"
                k6 run k6-tests/mock-demo.js
            fi
            ;;
        *)
            echo -e "${RED}‚ùå Unknown test type: ${test_type}${NC}"
            echo "Available types: gradual, spike, stress, auto-scaling-demo"
            return 1
            ;;
    esac
}

# Function to monitor scaling in real-time
monitor_scaling() {
    echo -e "${BLUE}üëÄ Real-time Scaling Monitor${NC}"
    echo "============================"
    echo -e "${YELLOW}Press Ctrl+C to stop monitoring${NC}"
    echo ""
    
    if [[ "$DEMO_MODE" == "kubernetes" ]]; then
        # Monitor HPA and pod scaling
        watch -n 2 "echo -e '${CYAN}HPA Status:${NC}'; kubectl get hpa -n ${NAMESPACE}; echo ''; echo -e '${CYAN}Pod Status:${NC}'; kubectl get pods -n ${NAMESPACE} -l 'app in (product-service,user-service,checkout-service,analytics-service)' --sort-by=.metadata.creationTimestamp"
    else
        # Monitor Docker container resources
        watch -n 2 "echo -e '${CYAN}Container Resource Usage:${NC}'; docker stats --no-stream --format 'table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}' | grep -E '(product|user|checkout|analytics)'"
    fi
}

# Interactive demo function
interactive_demo() {
    echo -e "${PURPLE}üé≠ Interactive Auto-Scaling Demo${NC}"
    echo "================================="
    echo ""
    echo "This demo will show you:"
    echo "1. üìä Current service status"
    echo "2. üñ•Ô∏è  Monitoring dashboards"
    echo "3. üî• Load testing scenarios"
    echo "4. üëÄ Real-time scaling observation"
    echo ""
    
    read -p "Press Enter to start the demo..."
    
    # Step 1: Show current status
    echo -e "\n${BLUE}Step 1: Current Service Status${NC}"
    show_service_status
    read -p "Press Enter to open monitoring dashboards..."
    
    # Step 2: Open monitoring
    echo -e "\n${BLUE}Step 2: Opening Monitoring Dashboards${NC}"
    open_monitoring
    read -p "Press Enter to start load testing..."
    
    # Step 3: Load testing menu
    echo -e "\n${BLUE}Step 3: Load Testing Scenarios${NC}"
    echo "Choose a load test scenario:"
    echo "1) Auto-Scaling Demo (optimized for presentation - RECOMMENDED)"
    echo "2) Gradual Load Increase"
    echo "3) Spike Test (sudden traffic burst)"
    echo "4) Stress Test (sustained high load)"
    echo "5) Skip load testing"
    
    read -p "Enter your choice (1-5): " choice
    
    case $choice in
        1)
            run_load_test "auto-scaling-demo" &
            LOAD_TEST_PID=$!
            ;;
        2)
            run_load_test "gradual" &
            LOAD_TEST_PID=$!
            ;;
        3)
            run_load_test "spike" &
            LOAD_TEST_PID=$!
            ;;
        4)
            run_load_test "stress" &
            LOAD_TEST_PID=$!
            ;;
        5)
            echo -e "${YELLOW}Skipping load test${NC}"
            ;;
        *)
            echo -e "${YELLOW}Invalid choice, running auto-scaling demo test${NC}"
            run_load_test "auto-scaling-demo" &
            LOAD_TEST_PID=$!
            ;;
    esac
    
    # Step 4: Monitor scaling
    echo -e "\n${BLUE}Step 4: Real-time Scaling Monitor${NC}"
    echo -e "${GREEN}üéØ Now watch the magic happen!${NC}"
    echo ""
    echo -e "${YELLOW}What to observe:${NC}"
    echo "  - CPU/Memory usage increasing"
    echo "  - New pods being created (Kubernetes) or container resource scaling"
    echo "  - Load balancing across multiple instances"
    echo "  - Response times staying stable despite increased load"
    echo ""
    
    if [[ "$DEMO_MODE" == "kubernetes" ]]; then
        echo -e "${CYAN}Watch these Grafana panels:${NC}"
        echo "  - Service Health Status"
        echo "  - Request Rate by Service"
        echo "  - System Resource Usage"
        echo "  - HPA metrics (if configured)"
    else
        echo -e "${CYAN}Watch these Docker metrics:${NC}"
        echo "  - Container CPU and Memory usage"
        echo "  - Network I/O"
        echo "  - Container scaling behavior"
    fi
    echo ""
    
    read -p "Press Enter to start monitoring..."
    monitor_scaling
}

# Function to generate presentation script
generate_presentation_script() {
    echo -e "${BLUE}üìù Generating Presentation Script${NC}"
    
    cat > "presentation-script.md" << 'EOF'
# Auto-Scaling Demo Presentation Script

## üéØ Demo Objective
Demonstrate how our High-Availability architecture automatically scales to handle varying loads while maintaining performance and reliability.

## üìã Pre-Demo Checklist
- [ ] All services running and healthy
- [ ] Monitoring dashboards accessible
- [ ] Load testing tools ready
- [ ] Audience can see both terminal and browser

## üé≠ Presentation Flow

### 1. Introduction (2 minutes)
**Say:** "Today I'll show you how our microservices architecture automatically adapts to load changes."

**Show:** Current service status
```bash
./demo-docs/scripts/auto-scaling-demo.sh docker  # or kubernetes
```

**Explain:**
- Current replica count
- Resource usage baseline
- Health status of all services

### 2. Monitoring Setup (2 minutes)
**Say:** "Let's open our real-time monitoring to observe the scaling behavior."

**Show:** Grafana dashboards
- Point out key metrics: Request Rate, CPU Usage, Memory Usage
- Explain the different panels
- Show current baseline metrics

**Key Points:**
- Real-time metric collection
- Visual representation of system health
- Automatic alerting capabilities

### 3. Load Testing Scenarios (8 minutes)

#### Scenario A: Auto-Scaling Demo Test (RECOMMENDED)
**Say:** "I'll run our specialized auto-scaling demonstration that simulates realistic user behavior."

**Show:** Start auto-scaling demo test
- Explain the load pattern (gradual increase to peak, then scale down)
- Point out metrics changing in real-time
- Highlight when scaling triggers occur

**Observe Together:**
- Response times staying stable
- CPU/Memory increasing
- New instances being created
- Load distribution across instances

#### Alternative Scenarios:
- **Gradual Load:** Traditional traffic pattern
- **Spike Test:** Sudden traffic bursts
- **Stress Test:** Sustained high load

### 4. Real-time Analysis (3 minutes)
**Say:** "Let's analyze what we just observed."

**Highlight:**
- Scaling thresholds (CPU > 70%, Memory > 80%)
- Response time consistency
- Automatic load distribution
- System self-healing capabilities

**Technical Details:**
- Horizontal Pod Autoscaler (HPA) configuration
- Resource requests and limits
- Health checks and readiness probes

## üéØ Key Messages

### For Technical Audience:
- "Our HPA scales based on CPU and memory metrics"
- "We maintain sub-second response times even under 10x load"
- "Zero-downtime scaling with rolling updates"
- "Kubernetes-native scaling with proper resource governance"

### For Business Audience:
- "System automatically handles traffic spikes without manual intervention"
- "Consistent user experience regardless of load"
- "Cost-efficient scaling - only use resources when needed"
- "99.9% uptime through redundancy and auto-scaling"

## üìä Expected Results to Highlight

| Metric | Baseline | Under Load | Scaled State |
|--------|----------|------------|--------------|
| Response Time | <100ms | <200ms | <150ms |
| CPU Usage | 10-20% | 80-90% | 40-60% |
| Replica Count | 3 | 3 | 8-12 |
| Request Rate | 10 RPS | 100+ RPS | 100+ RPS |

## üîß Troubleshooting

### If scaling doesn't trigger:
- Check HPA configuration
- Verify metrics server is running
- Ensure sufficient cluster resources

### If response times degrade:
- Point out this demonstrates the need for scaling
- Show how additional replicas improve performance

### If demo environment is slow:
- Explain this is expected in development
- Emphasize production performance characteristics

## üé§ Q&A Preparation

**Q: How fast does scaling happen?**
A: "Typically 30-60 seconds for new pods to be ready and serving traffic."

**Q: What triggers scaling decisions?**
A: "We use CPU and memory thresholds, but can extend to custom metrics like request queue length."

**Q: How do you prevent flapping?**
A: "We use stabilization windows and conservative scaling policies."

**Q: What about cost implications?**
A: "Auto-scaling ensures we only pay for what we use, with built-in safeguards against runaway scaling."

## üéØ Call to Action
"This demonstrates our platform's ability to maintain performance under any load condition while optimizing costs through intelligent scaling."
EOF

    echo -e "${GREEN}‚úÖ Presentation script generated: presentation-script.md${NC}"
}

# Function to create custom load test
create_custom_load_test() {
    echo -e "${BLUE}üèóÔ∏è  Creating custom load test scenarios${NC}"
    
    mkdir -p k6-tests
    
    # Spike test
    cat > "k6-tests/spike-test.js" << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate } from 'k6/metrics';

export let errorRate = new Rate('errors');

export let options = {
  stages: [
    { duration: '10s', target: 1 },   // Ramp up slowly
    { duration: '10s', target: 100 }, // Spike to 100 users
    { duration: '30s', target: 100 }, // Maintain spike
    { duration: '10s', target: 1 },   // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<2000'], // 95% of requests under 2s
    errors: ['rate<0.1'],              // Error rate under 10%
  },
};

const PRODUCT_URL = __ENV.PRODUCT_URL || 'http://localhost:8080';
const USER_URL = __ENV.USER_URL || 'http://localhost:8081';
const CHECKOUT_URL = __ENV.CHECKOUT_URL || 'http://localhost:8082';

export default function() {
  // Test different services randomly
  const services = [
    { name: 'Product', url: `${PRODUCT_URL}/api/v1/products` },
    { name: 'User', url: `${USER_URL}/api/v1/users` },
    { name: 'Checkout', url: `${CHECKOUT_URL}/health` },
  ];
  
  const service = services[Math.floor(Math.random() * services.length)];
  
  let response = http.get(service.url);
  
  let success = check(response, {
    [`${service.name} - status is 200`]: (r) => r.status === 200,
    [`${service.name} - response time < 2000ms`]: (r) => r.timings.duration < 2000,
  });
  
  errorRate.add(!success);
  
  sleep(Math.random() * 2 + 0.5); // Random sleep between 0.5-2.5s
}
EOF

    # Stress test
    cat > "k6-tests/stress-test.js" << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Counter, Rate } from 'k6/metrics';

export let errorRate = new Rate('errors');
export let requests = new Counter('requests');

export let options = {
  stages: [
    { duration: '2m', target: 50 },   // Ramp up to 50 users
    { duration: '5m', target: 50 },   // Stay at 50 users for 5 minutes
    { duration: '2m', target: 0 },    // Ramp down to 0 users
  ],
};

const PRODUCT_URL = __ENV.PRODUCT_URL || 'http://localhost:8080';
const USER_URL = __ENV.USER_URL || 'http://localhost:8081';
const CHECKOUT_URL = __ENV.CHECKOUT_URL || 'http://localhost:8082';

export default function() {
  requests.add(1);
  
  // Simulate realistic user behavior
  let responses = http.batch([
    ['GET', `${PRODUCT_URL}/api/v1/products`],
    ['GET', `${USER_URL}/health`],
    ['GET', `${CHECKOUT_URL}/health`],
  ]);
  
  responses.forEach((response, index) => {
    let success = check(response, {
      'status is 200': (r) => r.status === 200,
      'response time < 1000ms': (r) => r.timings.duration < 1000,
    });
    
    errorRate.add(!success);
  });
  
  sleep(1); // 1 second between iterations
}
EOF

    echo -e "${GREEN}‚úÖ Custom load tests created in k6-tests/${NC}"
}

# Main demo flow
main() {
    check_prerequisites
    create_custom_load_test
    
    case ${1:-"interactive"} in
        "docker"|"kubernetes"|"interactive"|"demo")
            start_services
            interactive_demo
            ;;
        "status")
            show_service_status
            ;;
        "monitor")
            monitor_scaling
            ;;
        "load-test")
            run_load_test ${2:-"auto-scaling-demo"}
            ;;
        "presentation")
            generate_presentation_script
            ;;
        "help"|"--help"|"-h")
            echo "Usage: $0 [MODE] [OPTION]"
            echo ""
            echo "Modes:"
            echo "  docker      - Interactive demo with Docker (default)"
            echo "  kubernetes  - Interactive demo with Kubernetes"
            echo "  interactive - Same as docker mode"
            echo "  status      - Show current service status"
            echo "  monitor     - Start real-time monitoring"
            echo "  load-test   - Run load test [auto-scaling-demo|gradual|spike|stress]"
            echo "  presentation - Generate presentation script"
            echo ""
            echo "Examples:"
            echo "  $0 docker              # Interactive demo with Docker"
            echo "  $0 kubernetes         # Interactive demo with Kubernetes"
            echo "  $0 load-test spike    # Run spike load test"
            echo "  $0 presentation       # Generate presentation guide"
            ;;
        *)
            echo -e "${RED}Unknown option: $1${NC}"
            echo "Use '$0 help' for usage information"
            exit 1
            ;;
    esac
}

# Cleanup function
cleanup() {
    echo -e "\n${YELLOW}üßπ Cleaning up...${NC}"
    
    # Kill background processes
    if [[ -n "${LOAD_TEST_PID:-}" ]]; then
        kill ${LOAD_TEST_PID} 2>/dev/null || true
    fi
    
    # Kill port forwarding processes
    pkill -f "kubectl port-forward" 2>/dev/null || true
    
    echo -e "${GREEN}‚úÖ Cleanup completed${NC}"
}

# Set up signal handling
trap cleanup EXIT INT TERM

# Run main function
main "$@" 